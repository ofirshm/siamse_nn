{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Siamese Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/zibV5.jpg\" width=\"500\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scarced data:\n",
    "## alphabets\n",
    "## Finger prints\n",
    "## Faces\n",
    "## handwritten checks\n",
    "## After being first introduced in 2016, Twin fully convolutional network has been used in many High-performance Real-time Object Tracking Neural Networks. Like CFnet,[7] StructSiam,[8] SiamFC-tri,[9] DSiam,[10] SA-Siam,[11] SiamRPN,[12] DaSiamRPN,[13] Cascaded SiamRPN,[14] SiamMask,[15] SiamRPN++,[16] Deeper and Wider SiamRPN.[17]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](Images/siames.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Recognition Using Siamese Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will understand the siamese network by building the face recognition model. The objective of our network is to understand whether two faces are similar or dissimilar. We use AT & T's the Database of Faces which can be downloaded from here (https://www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.html)\n",
    "\n",
    "Once you have downloaded and extracted the archive, you can see the folders like s1, s2 up to s40 as shown here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](Images/1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of these folders has 10 different images of a single person taken from various angles. For an instance, let us open folder s1. As you can see, there are 10 different images of a single person:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](Images/2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will open and check folder s13,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](Images/3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we know that siamese networks require inputs as a pair along with the label, we have to create our data in such a way. So we will take two images randomly from the same folder and mark it as a genuine pair and we will take a single image from two different folders and mark them as an imposite pair. A sample data is shown in the below figure, as you can notice a genuine pair has images of the same person and imposite pair has images of a different person. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](Images/5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have our data as pairs along with their labels, we train our siamese network. From the image pair, we feed one image to the network A and another image to the network B. The role of these two networks is only to extract the feature vectors. So, we use two convolution layers with relu activations for extracting the features. Once we have learned the feature, we feed the resultant feature vector from both of the networks to the energy function which measures the similarity,  we use Euclidean distance as our energy function. So, we train our network by feeding the image pair to learn the semantic similarity between them.  Now, we will see this step by step. \n",
    "\n",
    "\n",
    "First, we will import the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Offir\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:115: UserWarning: Numpy 1.13.3 or above is required for this version of scipy (detected version 1.12.1)\n",
      "  UserWarning)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "module compiled against API version 0xb but this version of numpy is 0xa",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;31mRuntimeError\u001b[0m: module compiled against API version 0xb but this version of numpy is 0xa"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "numpy.core.multiarray failed to import",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-59ceca32b4bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mActivation\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Offir\\anaconda3\\lib\\site-packages\\sklearn\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m__check_build\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mclone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_show_versions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mshow_versions\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Offir\\anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_IS_32BIT\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m _DEFAULT_TAGS = {\n",
      "\u001b[0;32mC:\\Users\\Offir\\anaconda3\\lib\\site-packages\\sklearn\\utils\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0missparse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mmurmurhash\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmurmurhash3_32\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Offir\\anaconda3\\lib\\site-packages\\scipy\\sparse\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mcsr\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mcsc\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mlil\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Offir\\anaconda3\\lib\\site-packages\\scipy\\sparse\\csr.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mspmatrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m from ._sparsetools import (csr_tocsc, csr_tobsr, csr_count_blocks,\n\u001b[0m\u001b[1;32m     14\u001b[0m                            get_csr_submatrix)\n\u001b[1;32m     15\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0msputils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mupcast\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mget_index_dtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: numpy.core.multiarray failed to import"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import backend as K\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Input, Lambda, Dense, Dropout, Convolution2D, MaxPooling2D, Flatten\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we define a function for reading our input image. The function read_image takes input as an image and returns the numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_image(filename, byteorder='>'):\n",
    "    \n",
    "    #first we read the image, as a raw file to the buffer\n",
    "    with open(filename, 'rb') as f:\n",
    "        buffer = f.read()\n",
    "    \n",
    "    #using regex, we extract the header, width, height and maxval of the image\n",
    "    header, width, height, maxval = re.search(\n",
    "        b\"(^P5\\s(?:\\s*#.*[\\r\\n])*\"\n",
    "        b\"(\\d+)\\s(?:\\s*#.*[\\r\\n])*\"\n",
    "        b\"(\\d+)\\s(?:\\s*#.*[\\r\\n])*\"\n",
    "        b\"(\\d+)\\s(?:\\s*#.*[\\r\\n]\\s)*)\", buffer).groups()\n",
    "    \n",
    "    #then we convert the image to numpy array using np.frombuffer which interprets buffer as one dimensional array\n",
    "    return np.frombuffer(buffer,\n",
    "                            dtype='u1' if int(maxval) < 256 else byteorder+'u2',\n",
    "                            count=int(width)*int(height),\n",
    "                            offset=len(header)\n",
    "                            ).reshape((int(height), int(width)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For an example, Let us open one image,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.open(\"data/orl_faces/s1/1.pgm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we feed this image to our read_image function, it will return as the numpy array,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img = read_image('data/orl_faces/s1/1.pgm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we define another function get_data for generating our data. As we know, for the Siamese network, data should be in the form of pairs (genuine and imposite) with a binary label.\n",
    "\n",
    "First, we read the images (img1, img2) from the same directory and store them in the x_genuine_pair array and assign y_genuine to 1. Next, we read the images (img1, img2) from the different directory and store them in the x_imposite pair and assign y_imposite to 0.\n",
    "\n",
    "Finally, we concatenate both x_genuine_pair, x_imposite to X and y_genuine, y_imposite to Y:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "size = 2\n",
    "total_sample_size = 10000\n",
    "\n",
    "\n",
    "def get_data(size, total_sample_size):\n",
    "    #read the image\n",
    "    image = read_image('data/orl_faces/s' + str(1) + '/' + str(1) + '.pgm', 'rw+')\n",
    "    #reduce the size\n",
    "    image = image[::size, ::size]\n",
    "    #get the new size\n",
    "    dim1 = image.shape[0]\n",
    "    dim2 = image.shape[1]\n",
    "\n",
    "    count = 0\n",
    "    \n",
    "    #initialize the numpy array with the shape of [total_sample, no_of_pairs, dim1, dim2]\n",
    "    x_geuine_pair = np.zeros([total_sample_size, 2, 1, dim1, dim2])  # 2 is for pairs\n",
    "    y_genuine = np.zeros([total_sample_size, 1])\n",
    "    \n",
    "    for i in range(40):\n",
    "        for j in range(int(total_sample_size/40)):\n",
    "            ind1 = 0\n",
    "            ind2 = 0\n",
    "            \n",
    "            #read images from same directory (genuine pair)\n",
    "            while ind1 == ind2:\n",
    "                ind1 = np.random.randint(10)\n",
    "                ind2 = np.random.randint(10)\n",
    "            \n",
    "            # read the two images\n",
    "            img1 = read_image('data/orl_faces/s' + str(i+1) + '/' + str(ind1 + 1) + '.pgm', 'rw+')\n",
    "            img2 = read_image('data/orl_faces/s' + str(i+1) + '/' + str(ind2 + 1) + '.pgm', 'rw+')\n",
    "            \n",
    "            #reduce the size\n",
    "            img1 = img1[::size, ::size]\n",
    "            img2 = img2[::size, ::size]\n",
    "            \n",
    "            #store the images to the initialized numpy array\n",
    "            x_geuine_pair[count, 0, 0, :, :] = img1\n",
    "            x_geuine_pair[count, 1, 0, :, :] = img2\n",
    "            \n",
    "            #as we are drawing images from the same directory we assign label as 1. (genuine pair)\n",
    "            y_genuine[count] = 1\n",
    "            count += 1\n",
    "\n",
    "    count = 0\n",
    "    x_imposite_pair = np.zeros([total_sample_size, 2, 1, dim1, dim2])\n",
    "    y_imposite = np.zeros([total_sample_size, 1])\n",
    "    \n",
    "    for i in range(int(total_sample_size/10)):\n",
    "        for j in range(10):\n",
    "            \n",
    "            #read images from different directory (imposite pair)\n",
    "            while True:\n",
    "                ind1 = np.random.randint(40)\n",
    "                ind2 = np.random.randint(40)\n",
    "                if ind1 != ind2:\n",
    "                    break\n",
    "                    \n",
    "            img1 = read_image('data/orl_faces/s' + str(ind1+1) + '/' + str(j + 1) + '.pgm', 'rw+')\n",
    "            img2 = read_image('data/orl_faces/s' + str(ind2+1) + '/' + str(j + 1) + '.pgm', 'rw+')\n",
    "\n",
    "            img1 = img1[::size, ::size]\n",
    "            img2 = img2[::size, ::size]\n",
    "\n",
    "            x_imposite_pair[count, 0, 0, :, :] = img1\n",
    "            x_imposite_pair[count, 1, 0, :, :] = img2\n",
    "            #as we are drawing images from the different directory we assign label as 0. (imposite pair)\n",
    "            y_imposite[count] = 0\n",
    "            count += 1\n",
    "            \n",
    "    #now, concatenate, genuine pairs and imposite pair to get the whole data\n",
    "    X = np.concatenate([x_geuine_pair, x_imposite_pair], axis=0)/255\n",
    "    Y = np.concatenate([y_genuine, y_imposite], axis=0)\n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we generate our data and check our data size. As you can see we have 20,000 data points, out of these 10,000 are genuine pairs and 10,000 are imposite pairs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, Y = get_data(size, total_sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we split our data for training and testing with 75% training and 25% testing proportions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that, we have successfully generated our data, we build our siamese network. First, we define the base network which is basically a convolutional network used for feature extraction. We build two convolutional layers with rectified linear unit (ReLU) activations and max pooling followed by flat layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_base_network(input_shape):\n",
    "    \n",
    "    seq = Sequential()\n",
    "    \n",
    "    nb_filter = [6, 12]\n",
    "    kernel_size = 3\n",
    "    \n",
    "    \n",
    "    #convolutional layer 1\n",
    "    seq.add(Convolution2D(nb_filter[0], kernel_size, kernel_size, input_shape=input_shape,\n",
    "                          border_mode='valid', dim_ordering='th'))\n",
    "    seq.add(Activation('relu'))\n",
    "    seq.add(MaxPooling2D(pool_size=(2, 2)))  \n",
    "    seq.add(Dropout(.25))\n",
    "    \n",
    "    #convolutional layer 2\n",
    "    seq.add(Convolution2D(nb_filter[1], kernel_size, kernel_size, border_mode='valid', dim_ordering='th'))\n",
    "    seq.add(Activation('relu'))\n",
    "    seq.add(MaxPooling2D(pool_size=(2, 2), dim_ordering='th')) \n",
    "    seq.add(Dropout(.25))\n",
    "\n",
    "    #flatten \n",
    "    seq.add(Flatten())\n",
    "    seq.add(Dense(128, activation='relu'))\n",
    "    seq.add(Dropout(0.1))\n",
    "    seq.add(Dense(50, activation='relu'))\n",
    "    return seq\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we feed the image pair, to the base network, which will return the embeddings that is, feature vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_dim = x_train.shape[2:]\n",
    "img_a = Input(shape=input_dim)\n",
    "img_b = Input(shape=input_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_network = build_base_network(input_dim)\n",
    "feat_vecs_a = base_network(img_a)\n",
    "feat_vecs_b = base_network(img_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These feat_vecs_a and feat_vecs_b are the feature vectors of our image pair. Next, we feed this feature vectors to the energy function to compute the distance between them, we use Euclidean distance as our energy function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def euclidean_distance(vects):\n",
    "    x, y = vects\n",
    "    return K.sqrt(K.sum(K.square(x - y), axis=1, keepdims=True))\n",
    "\n",
    "\n",
    "def eucl_dist_output_shape(shapes):\n",
    "    shape1, shape2 = shapes\n",
    "    return (shape1[0], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "distance = Lambda(euclidean_distance, output_shape=eucl_dist_output_shape)([feat_vecs_a, feat_vecs_b])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Now, we set the epoch length to 13 and we use RMS prop for optimization and define our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = 13\n",
    "rms = RMSprop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(input=[img_a, img_b], output=distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define our loss function as contrastive_loss function and compile the model. \n",
    "\n",
    "Without maths, 2 main points:\n",
    "\n",
    "it is a Distance-based Loss function (as opposed to prediction error-based Loss functions like Logistic loss or Hinge loss used in Classification).\n",
    "\n",
    "Like any distance-based loss, it tries to ensure that semantically similar examples are embedded close together. It is calculated on Pairs (other popular distance-based Loss functions are Triplet & Centre Loss, calculated on Triplets and Pointwise respectively)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def contrastive_loss(y_true, y_pred):\n",
    "    margin = 1\n",
    "    return K.mean(y_true * K.square(y_pred) + (1 - y_true) * K.square(K.maximum(margin - y_pred, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss=contrastive_loss, optimizer=rms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()\n",
    "from keras.utils import plot_model\n",
    "plot_model(model, to_file='model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_1 = x_train[:, 0]\n",
    "img2 = x_train[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit([img_1, img2], y_train, validation_split=.25,\n",
    "          batch_size=128, verbose=2, nb_epoch=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we make predictions with test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = model.predict([x_test[:, 0], x_test[:, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_accuracy(predictions, labels):\n",
    "    return labels[predictions.ravel() < 0.5].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we check our model accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "compute_accuracy(pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K \n",
    "\n",
    "\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
